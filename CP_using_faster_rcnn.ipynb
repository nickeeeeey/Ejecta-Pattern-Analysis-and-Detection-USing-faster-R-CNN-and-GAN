{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8608dff2",
        "scrolled": true,
        "outputId": "5491a723-4462-492d-eb40-51e3900c20b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 414593, done.\u001b[K\n",
            "remote: Counting objects: 100% (23757/23757), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1889/1889), done.\u001b[K\n",
            "remote: Total 414593 (delta 22931), reused 22190 (delta 21857), pack-reused 390836\u001b[K\n",
            "Receiving objects: 100% (414593/414593), 796.98 MiB | 35.82 MiB/s, done.\n",
            "Resolving deltas: 100% (385153/385153), done.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (3.0.4)\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-ospy3aso\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-ospy3aso\n",
            "  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0) (67.7.2)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0) (3.0.4)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools==2.0) (1.16.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp310-cp310-linux_x86_64.whl size=375264 sha256=7b3a16f66b83c19f2e8ce0a94532e079b0e4b0bd150f02c4c4c4c7b90f622c22\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wihnb0xp/wheels/39/61/b4/480fbddb4d3d6bc34083e7397bc6f5d1381f79acc68e9f3511\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.7\n",
            "    Uninstalling pycocotools-2.0.7:\n",
            "      Successfully uninstalled pycocotools-2.0.7\n",
            "Successfully installed pycocotools-2.0\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.11.3)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.1)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.8.1.78)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (4.5.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (3.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2.31.5)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (23.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.2.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n",
            "Installing collected packages: opencv-python\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.8.0.76\n",
            "    Uninstalling opencv-python-4.8.0.76:\n",
            "      Successfully uninstalled opencv-python-4.8.0.76\n",
            "Successfully installed opencv-python-4.8.1.78\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!git clone https://github.com/pytorch/vision.git\n",
        "!cd vision\n",
        "!git checkout v0.8.2\n",
        "\n",
        "!cp ./vision/references/detection/utils.py ./\n",
        "!cp ./vision/references/detection/transforms.py ./\n",
        "!cp ./vision/references/detection/coco_eval.py ./\n",
        "!cp ./vision/references/detection/engine.py ./\n",
        "!cp ./vision/references/detection/coco_utils.py ./\n",
        "\n",
        "!pip install cython\n",
        "\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "!pip install -U albumentations\n",
        "!pip install -U opencv-python\n",
        "\n"
      ],
      "id": "8608dff2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYHAbfbOs2qX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e1e28be-999f-4867-8c4e-3cd4541e2a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive, files\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "file_path = '/content/drive/MyDrive/dataset.zip'"
      ],
      "id": "sYHAbfbOs2qX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18_4l1GZuOpm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e17bfad9-f490-4c7b-f9aa-4bba9cab9376"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/dataset.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "\n",
        "import shutil\n",
        "\n",
        "# Specify the source and destination paths\n",
        "source_path = r'/content/drive/MyDrive/dataset.zip'  # Path to the file in Google Drive\n",
        "destination_path = '/content/dataset.zip'  # Path in Colab environment\n",
        "\n",
        "# Copy the file from Google Drive to Colab\n",
        "shutil.copy(source_path, destination_path)\n"
      ],
      "id": "18_4l1GZuOpm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmlvHM68uvGz"
      },
      "outputs": [],
      "source": [
        "\n",
        "!unzip dataset.zip"
      ],
      "id": "pmlvHM68uvGz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0w4tPd_ntzqn"
      },
      "outputs": [],
      "source": [
        "\n",
        "!cp -a craters/train/images/. ./train/images/\n",
        "!cp -a craters/valid/images/. ./train/images/\n",
        "!cp -a craters/train/labels/. ./train/labels/\n",
        "!cp -a craters/valid/labels/. ./train/labels/"
      ],
      "id": "0w4tPd_ntzqn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e5d58ef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "import albumentations as A\n",
        "import cv2\n",
        "import time\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from  sklearn.model_selection import KFold\n",
        "import random"
      ],
      "id": "1e5d58ef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "865396e8"
      },
      "outputs": [],
      "source": [
        "class CraterDataset(object):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(self.root, \"images\"))))\n",
        "        self.annots = list(sorted(os.listdir(os.path.join(self.root, \"labels\"))))\n",
        "        self.classes = ['Background','Crater']\n",
        "\n",
        "\n",
        "    def convert_box_cord(self,bboxs, format_from, format_to, img_shape):\n",
        "        if format_from == 'normxywh':\n",
        "            if format_to == 'xyminmax':\n",
        "                xw = bboxs[:, (1, 3)] * img_shape[1]\n",
        "                yh = bboxs[:, (2, 4)] * img_shape[0]\n",
        "                xmin = xw[:, 0] - xw[:, 1] / 2\n",
        "                xmax = xw[:, 0] + xw[:, 1] / 2\n",
        "                ymin = yh[:, 0] - yh[:, 1] / 2\n",
        "                ymax = yh[:, 0] + yh[:, 1] / 2\n",
        "                coords_converted = np.column_stack((xmin, ymin, xmax, ymax))\n",
        "\n",
        "        return coords_converted\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and boxes\n",
        "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
        "        annot_path = os.path.join(self.root, \"labels\", self.annots[idx])\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        img= img/255.0\n",
        "\n",
        "\n",
        "        if os.path.getsize(annot_path) != 0:\n",
        "            bboxs = np.loadtxt(annot_path, ndmin=2)\n",
        "            bboxs = self.convert_box_cord(bboxs, 'normxywh', 'xyminmax', img.shape)\n",
        "            num_objs = len(bboxs)\n",
        "            bboxs = torch.as_tensor(bboxs, dtype=torch.float32)\n",
        "\n",
        "            labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "\n",
        "            iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "        else:\n",
        "            bboxs = torch.as_tensor([[0, 0, 640, 640]], dtype=torch.float32)\n",
        "            labels = torch.zeros((1,), dtype=torch.int64)\n",
        "            iscrowd = torch.zeros((1,), dtype=torch.int64)\n",
        "\n",
        "        area = (bboxs[:, 3] - bboxs[:, 1]) * (bboxs[:, 2] - bboxs[:, 0])\n",
        "        image_id = torch.tensor([idx])\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = bboxs\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            sample = self.transforms(image=img,\n",
        "                                     bboxes=target['boxes'],\n",
        "                                     labels=labels)\n",
        "        img = sample['image']\n",
        "        target['boxes'] = torch.tensor(sample['bboxes'])\n",
        "        target['labels'] = torch.tensor(sample['labels'])\n",
        "        if target['boxes'].ndim == 1:\n",
        "            target['boxes'] = torch.as_tensor([[0, 0, 640, 640]], dtype=torch.float32)\n",
        "            target['labels'] = torch.zeros((1,), dtype=torch.int64)\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n"
      ],
      "id": "865396e8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f9e3092"
      },
      "outputs": [],
      "source": [
        "def get_model_bbox(num_classes):\n",
        "\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model"
      ],
      "id": "4f9e3092"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f37fd06"
      },
      "outputs": [],
      "source": [
        "def get_transform(train):\n",
        "    if train:\n",
        "        return A.Compose([\n",
        "            # A.Flip(p=0.5),\n",
        "            # A.RandomResizedCrop(height=640,width=640,p=0.4),\n",
        "            # # A.Perspective(p=0.4),\n",
        "            # A.Rotate(p=0.5),\n",
        "            # # A.Transpose(p=0.3),\n",
        "            ToTensorV2(p=1.0)],\n",
        "            bbox_params=A.BboxParams(format='pascal_voc',min_visibility=0.4, label_fields=['labels']))\n",
        "    else:\n",
        "        return A.Compose([ToTensorV2(p=1.0)],\n",
        "                         bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.5, label_fields=['labels']))"
      ],
      "id": "2f37fd06"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff705782"
      },
      "outputs": [],
      "source": [
        "def reset_weights(m):\n",
        "  '''\n",
        "    Try resetting model weights to avoid\n",
        "    weight leakage.\n",
        "  '''\n",
        "  for layer in m.children():\n",
        "    if hasattr(layer, 'reset_parameters'):\n",
        "        print(f'Reset trainable parameters of layer = {layer}')\n",
        "        layer.reset_parameters()"
      ],
      "id": "ff705782"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35de739f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import math\n",
        "def plot_img_bbox(img, target):\n",
        "\n",
        "    fig, a = plt.subplots(1, 1)\n",
        "    fig.set_size_inches(5, 5)\n",
        "    a.imshow(img.permute((1,2,0)))\n",
        "    for box in (target['boxes']):\n",
        "        x, y, width, height = box[0], box[1], box[2] - box[0], box[3] - box[1]\n",
        "        rect = patches.Rectangle((x, y),\n",
        "                                 width, height,\n",
        "                                 edgecolor='b',\n",
        "                                 facecolor='none',\n",
        "                                 clip_on=False)\n",
        "\n",
        "        radius = round(math.sqrt(((width*height)*7/22)),2)\n",
        "        depth = round(radius/11,2)\n",
        "\n",
        "        a.annotate('Ejecta r=' + str(radius) + \" d=\" + str(depth), (x,y-20), color='blue', weight='bold',\n",
        "                   fontsize=10, ha='left', va='top')\n",
        "\n",
        "        # Draw the bounding box on top of the image\n",
        "        a.add_patch(rect)\n",
        "    plt.show()"
      ],
      "id": "35de739f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c55d4d9e"
      },
      "outputs": [],
      "source": [
        "dataset = CraterDataset('craters/train', get_transform(train=True))\n",
        "for i in random.sample(range(1, 100), 3):\n",
        "    img, target = dataset[i]\n",
        "    plot_img_bbox(img, target)"
      ],
      "id": "c55d4d9e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed59b01d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# train on the GPU or on the CPU, if a GPU is not available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "k_folds = 3\n",
        "num_epochs = 300\n",
        "\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "dataset = CraterDataset('craters/train', get_transform(train=True))\n",
        "dataset_val = CraterDataset('craters/train', get_transform(train=False))\n",
        "\n",
        "\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "# Start print\n",
        "print('--------------------------------')\n",
        "\n",
        "\n",
        "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    dataset_subset = torch.utils.data.Subset(dataset, list(train_ids))\n",
        "    dataset_val_subset = torch.utils.data.Subset(dataset_val, list(val_ids))\n",
        "\n",
        "    # define training and validation data loaders\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "            dataset_subset, batch_size=8, shuffle=True, num_workers=2,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    data_loader_val = torch.utils.data.DataLoader(\n",
        "        dataset_val_subset, batch_size=1, shuffle=False, num_workers=2,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "\n",
        "    model = get_model_bbox(num_classes)\n",
        "    model.to(device)\n",
        "\n",
        "    # construct an optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=0.005,  # Check if beneficial\n",
        "                                momentum=0.9, weight_decay=0)\n",
        "\n",
        "\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                    step_size=10,\n",
        "                                                    gamma=0.1)\n",
        "\n",
        "    # let's train!\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "\n",
        "\n",
        "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)\n",
        "\n",
        "        lr_scheduler.step()\n",
        "        # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_val, device=device)"
      ],
      "id": "ed59b01d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c3a5708",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "\n",
        "num_classes = 2\n",
        "# use our dataset and defined transformations\n",
        "dataset = CraterDataset('craters/train', get_transform(train=True))\n",
        "dataset_test = CraterDataset('craters/test', get_transform(train=False))\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=8, shuffle=True, num_workers=2,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=2,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "\n",
        "model = get_model_bbox(num_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0)\n",
        "\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=20,\n",
        "                                                gamma=0.2)\n",
        "\n",
        "\n",
        "result_mAP = []\n",
        "best_epoch = None\n",
        "\n",
        "# Let's train!\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "\n",
        "\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)\n",
        "\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    results =  evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "    result_mAP.append(results.coco_eval['bbox'].stats[1])\n",
        "\n",
        "    if result_mAP[-1] == max(result_mAP):\n",
        "        best_save_path = os.path.join(f'Crater_bestmodel_noaug_sgd(wd=0)_8batch-epoch{epoch}.pth')\n",
        "        torch.save(model.state_dict(), best_save_path)\n",
        "        best_epoch = int(epoch)\n",
        "        print(f'\\n\\nmodel from epoch number {epoch} saved!\\n result is {max(result_mAP)}\\n\\n')\n",
        "\n",
        "\n",
        "save_path = os.path.join(f'Crater_noaug_sgd_2batch-lastepoch{num_epochs-1}.pth')\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f'model from last epoch(no.{num_epochs-1}) saved')"
      ],
      "id": "5c3a5708"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "269be2f5"
      },
      "outputs": [],
      "source": [
        "dataset_test = CraterDataset('craters/test', get_transform(train=False))\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=2,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "model = get_model_bbox(num_classes)\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load(os.path.join(f'Crater_bestmodel_noaug_sgd(wd=0)_8batch-epoch{best_epoch}.pth'),map_location=device))\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "evaluate(model, data_loader_test, device=device)"
      ],
      "id": "269be2f5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d55e0222"
      },
      "outputs": [],
      "source": [
        "# Define colors for bounding boxes\n",
        "color_inference = np.array([0.0,0.0,255.0])\n",
        "color_label = np.array([255.0,0.0,0.0])\n",
        "\n",
        "\n",
        "detection_threshold = 0.7\n",
        "\n",
        "frame_count = 0\n",
        "\n",
        "total_fps = 0\n",
        "\n",
        "!mkdir ./results"
      ],
      "id": "d55e0222"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d26d8988"
      },
      "outputs": [],
      "source": [
        "for i,data in enumerate(data_loader_test):\n",
        "\n",
        "    image_name = 'image no:' + str(int(data[1][0]['image_id']))\n",
        "    model_image = data[0][0]\n",
        "    cv2_image = np.transpose(model_image.numpy()*255,(1, 2, 0)).astype(np.float32)\n",
        "    cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_RGB2BGR).astype(np.float32)\n",
        "\n",
        "    # add batch dimension\n",
        "    model_image = torch.unsqueeze(model_image, 0)\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(model_image.to(device))\n",
        "    end_time = time.time()\n",
        "    # get the current fps\n",
        "    fps = 1 / (end_time - start_time)\n",
        "\n",
        "    total_fps += fps\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
        "\n",
        "    if len(outputs[0]['boxes']) != 0:\n",
        "        boxes = outputs[0]['boxes'].data.numpy()\n",
        "        scores = outputs[0]['scores'].data.numpy()\n",
        "\n",
        "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
        "        scores = np.round(scores[scores >= detection_threshold],2)\n",
        "        draw_boxes = boxes.copy()\n",
        "\n",
        "\n",
        "        for j,box in enumerate(draw_boxes):\n",
        "            cv2.rectangle(cv2_image,\n",
        "                          (int(box[0]), int(box[1])),\n",
        "                          (int(box[2]), int(box[3])),\n",
        "                          color_inference, 2)\n",
        "            cv2.putText(img=cv2_image, text=\"Crater\",\n",
        "                        org=(int(box[0]), int(box[1] - 5)),\n",
        "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,\n",
        "                        thickness=1, lineType=cv2.LINE_AA)\n",
        "            cv2.putText(img=cv2_image, text=str(scores[j]),\n",
        "                        org=(int(box[0]), int(box[1] + 8)),\n",
        "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,\n",
        "                        thickness=1, lineType=cv2.LINE_AA)\n",
        "\n",
        "        # add boxes for labels\n",
        "        for box in data[1][0]['boxes']:\n",
        "            cv2.rectangle(cv2_image,\n",
        "                          (int(box[0]), int(box[1])),\n",
        "                          (int(box[2]), int(box[3])),\n",
        "                          color_label, 2)\n",
        "            cv2.putText(img=cv2_image, text=\"Label\",\n",
        "                        org=(int(box[0]), int(box[1] - 5)),\n",
        "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_label,\n",
        "                        thickness=1, lineType=cv2.LINE_AA)\n",
        "\n",
        "\n",
        "        # set size\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # convert color from CV2 BGR back to RGB\n",
        "        plt_image = cv2.cvtColor(cv2_image/255.0, cv2.COLOR_BGR2RGB)\n",
        "        plt.imshow(plt_image)\n",
        "        plt.show()\n",
        "        cv2.imwrite(f\"./results/{image_name}.jpg\", cv2_image)\n",
        "    print(f\"Image {i + 1} done...\")\n",
        "    print('-' * 50)\n",
        "print('TEST PREDICTIONS COMPLETE')\n",
        "\n",
        "avg_fps = total_fps / frame_count\n",
        "print(f\"Average FPS: {avg_fps:.3f}\")\n"
      ],
      "id": "d26d8988"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4639.876758,
      "end_time": "2023-04-20T19:06:30.190496",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-04-20T17:49:10.313738",
      "version": "2.3.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}